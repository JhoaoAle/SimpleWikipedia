{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8957b6-94ea-4bbf-a166-f01f4f18bf0f",
   "metadata": {},
   "source": [
    "## Installing and importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0674f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jhoao\\anaconda3\\envs\\ml\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a89ebf-f5b8-420d-b4db-bc0ac363e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af573d48-dd4b-4899-97bc-508d843d8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(self, text):\n",
    "    \"\"\"\n",
    "    Enhanced link extraction with better pattern matching\n",
    "    \"\"\"\n",
    "    links = set()\n",
    "    if not text:\n",
    "        return links\n",
    "        \n",
    "    # Improved regex pattern for Wikipedia links\n",
    "    link_pattern = re.compile(r'\\[\\[(.*?)\\]\\]')\n",
    "    for match in link_pattern.finditer(text):\n",
    "        link_content = match.group(1)\n",
    "        \n",
    "        # Skip empty links\n",
    "        if not link_content.strip():\n",
    "            continue\n",
    "            \n",
    "        # Handle piped links and section links\n",
    "        link = link_content.split('|')[0].split('#')[0].strip()\n",
    "        \n",
    "        # Handle namespace prefixes\n",
    "        if ':' in link:\n",
    "            namespace, rest = link.split(':', 1)\n",
    "            namespace = namespace.lower()\n",
    "            if namespace in {'category', 'file', 'image', 'template', 'user', \n",
    "                             'wikipedia', 'wp', 'project', 'mediawiki'}:\n",
    "                continue\n",
    "            link = rest  # Use the part after namespace\n",
    "        \n",
    "        if link:  # Add if not empty after processing\n",
    "            links.add(link)\n",
    "    \n",
    "    return list(links)\n",
    "\n",
    "def parse_dump(self, file_obj, max_articles=None, block_size=1000):\n",
    "    \"\"\"\n",
    "    More robust parsing with better memory management\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting to parse Wikipedia dump...\")\n",
    "        \n",
    "        decompressor = bz2.BZ2Decompressor()\n",
    "        buffer = b''\n",
    "        article_count = 0\n",
    "        \n",
    "        # Use incremental parser\n",
    "        parser = ET.XMLPullParser(['start', 'end'])\n",
    "        \n",
    "        while True:\n",
    "            chunk = file_obj.read(8192)\n",
    "            if not chunk:\n",
    "                # Final flush of decompressor\n",
    "                buffer += decompressor.flush()\n",
    "                if not buffer:\n",
    "                    break\n",
    "            \n",
    "            buffer += decompressor.decompress(chunk)\n",
    "            \n",
    "            while b'<page>' in buffer and b'</page>' in buffer:\n",
    "                page_start = buffer.find(b'<page>')\n",
    "                page_end = buffer.find(b'</page>') + len(b'</page>')\n",
    "                page_xml = buffer[page_start:page_end]\n",
    "                buffer = buffer[page_end:]\n",
    "                \n",
    "                try:\n",
    "                    parser.feed(page_xml)\n",
    "                    for event, elem in parser.read_events():\n",
    "                        if event == 'end' and elem.tag == 'page':\n",
    "                            yield from self._process_page(elem)\n",
    "                            article_count += 1\n",
    "                            elem.clear()  # Free memory\n",
    "                            \n",
    "                            # Check block size and max articles\n",
    "                            if article_count % block_size == 0:\n",
    "                                yield self._create_dataframes()\n",
    "                            if max_articles and article_count >= max_articles:\n",
    "                                return\n",
    "                except ET.ParseError as e:\n",
    "                    logger.warning(f\"XML parsing error: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Yield any remaining articles\n",
    "        if self.current_block_articles or self.current_block_links:\n",
    "            yield self._create_dataframes()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing dump file: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c603786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 18:43:10,273 - INFO - Using dump file URL: https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles.xml.bz2\n",
      "2025-05-06 18:43:10,276 - INFO - Starting download of Wikipedia dump file...\n",
      "100%|██████████| 321M/321M [02:09<00:00, 2.49MB/s] \n",
      "2025-05-06 18:45:19,716 - INFO - Download completed successfully.\n",
      "2025-05-06 18:45:19,719 - INFO - Starting to parse Wikipedia dump...\n",
      "2025-05-06 18:45:19,720 - ERROR - Error parsing dump file: no element found: line 1, column 0\n",
      "2025-05-06 18:45:19,722 - ERROR - Error in process_dump: no element found: line 1, column 0\n"
     ]
    },
    {
     "ename": "ParseError",
     "evalue": "no element found: line 1, column 0 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Jhoao\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[8], line 7\u001b[0m\n    articles_file, links_file = parser.process_dump(max_articles=1000, block_size=200)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[5], line 264\u001b[0m in \u001b[0;35mprocess_dump\u001b[0m\n    for articles_df, links_df in self.parse_dump(dump_file, max_articles, block_size):\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[5], line 99\u001b[0m in \u001b[0;35mparse_dump\u001b[0m\n    _, root = next(context)  # Get the root element\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Jhoao\\anaconda3\\envs\\ML\\lib\\xml\\etree\\ElementTree.py:1259\u001b[0m in \u001b[0;35miterator\u001b[0m\n    root = pullparser._close_and_return_root()\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\Jhoao\\anaconda3\\envs\\ML\\lib\\xml\\etree\\ElementTree.py:1302\u001b[1;36m in \u001b[1;35m_close_and_return_root\u001b[1;36m\n\u001b[1;33m    root = self._parser.close()\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m no element found: line 1, column 0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    parser = WikipediaDumpParser()\n",
    "    \n",
    "    # Process a small sample (1000 articles) for demonstration\n",
    "    # Remove max_articles parameter to process the entire dump\n",
    "    articles_file, links_file = parser.process_dump(max_articles=1000, block_size=200)\n",
    "    \n",
    "    # Load the results to verify\n",
    "    articles_df = pd.read_parquet(articles_file)\n",
    "    links_df = pd.read_parquet(links_file)\n",
    "    \n",
    "    print(\"\\nSample Articles:\")\n",
    "    print(articles_df.head())\n",
    "    \n",
    "    print(\"\\nSample Links:\")\n",
    "    print(links_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
